<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hands on Data Scraping</title>

        <link href="assets/css/prism.css" rel="stylesheet"/>
    <link rel="stylesheet" href="assets/css/main.css">
    
</head>

<body>

    <div class="nav">
        <ul id="nav-container" class="nav-container">
            <li class="nav-item"><a href="index.html">Home</a></li>
            <li class="nav-item"><a href="introduction.html">Intorduction</a></li>
            <li class="nav-item"><a href="tools.html">Tools</a></li>
            <li class="nav-item"><a href="html.html">HTML</a></li>
            <li class="nav-item"><a href="css.html">CSS</a></li>
            <li class="nav-item"><a href="javascript.html">JavaScript</a></li>
            <li class="nav-item"><a href="python.html">Python</a></li>
            <li class="nav-item"><a href="python-advanced.html">Python (advanced)</a></li>
            <li class="nav-item"><a href="scrapy.html">Scrapy</a></li>
            <li class="nav-item"><a href="selenium.html">Selenium</a></li>
            <li class="nav-item"><a href="case-studies.html">Case Studies</a></li>
        </ul>
    </div>

    <div class="container">
        <div class="section">
            <div class="title">Selenium</div>

            <div class="txt">
                Now when you are familiar with “Scrapy”, thus you know how to scrape anything from a static webpage it’s
                time to get into the world of dynamic pages, where data can be generated with the help of “JS” “events”
                (discussed earlier).
            </div>

            <div class="txt">
                “Selenium” comes into play, it is designed for doing automated actions inside of a browser with the help
                of code. You write a piece of code which can open up your browser, navigate to your favorite grocery
                website and order food for you. How cool is that, huh ?
            </div>

            <div class="txt">
                With this motivation let us get into it and install the package and drivers needed. You can find a
                simple guide here <sup class="ref" src="https://selenium-python.readthedocs.io/installation.html"></sup>:
            </div>


            <div class="txt">
                Hopefully now you have “Selenium” installed on your machine, so let’s run the following code:
            </div>


            <pre><code class="language-py">from selenium import webdriver

browser = webdriver.Chrome()
browser.maximize_window()

browser.get('https://buy.am/en/carrefour/ready-meals/')</code></pre>


            <div class="txt">
                First we import the webdriver from selenium <sup class="ref" src="https://selenium-python.readthedocs.io/getting-started.html"></sup>, then we define browser variable which is a Chrome
                webdriver, so it will be using Chrome as your browsers, and then we give the url of “buy.am” to
                navigate. Seems scary, doesn’t it.
            </div>

            <div class="txt">
                Now let’s say you remember that at the end of the page there is a meal you really like and you don’t
                want every time to scroll down there, but want to automate that process.
            </div>

            <pre><code class="language-py">import time

time.sleep(3)
current_height = 0

while current_height != browser.execute_script("return document.body.scrollHeight;"):
    browser.execute_script("window.scrollTo(0,document.body.scrollHeight);")
    current_height = browser.execute_script("return document.body.scrollHeight;")
    time.sleep(1)</code></pre>


            <div class="txt">
                Here we import “time” and immediately use it. We call the sleep method which just pauses the code and
                waits for the web page to load.
            </div>

            <div class="note">
                <span class="note-keyword">Note</span>: All “sleep” methods called in our examples are only for the web
                page to be loaded, those numbers
                might need to be adjusted(increase/decrease) to satisfy your needs based on your connection speed.
            </div>

            <div class="txt">
                After which we define a variable called “current_height” it will hold the height from the top of the
                page till the end of it (in the beginning it will be 0, cause we are at the top). After which we are
                scrolling the page by executing “JS” code and updating the “current_height” value while there is a place
                to scroll.
            </div>

            <div class="txt">
                Now when you have all the elements in the webpage loaded you can scrape them with the help of “Scrapy”,
                just like you did before, or by using “Selenium” functionality.
            </div>

            <div class="txt">
                But, your favorite meal might not be at the end, other somewhere in the middle, so what should we do ?
                We can find the item which has a specific name in the page, like this:
            </div>


            <pre><code class="language-py">from selenium.webdriver.common.by import By

add_to_bucket = browser.find_element(By.XPATH, '//a[contains(text(), "Cabbage Salad")]/../div[@class="product-actions "]//button')

add_to_bucket.click()</code></pre>


            <div class="txt">
                Here we import “By” module from “selenium” and then use it inside of “find_element” function to specify that
                we want to find an element by “XPath” (you can select with css and many more). The element that we want
                should be an “a” tag which text should contain “Cabbage Salad” text, and we are interested in a button
                which is in a “div” with class “product-actions” which is sibling to the “a” tag. In the end we click on
                it. You should notice the bucket size to be increased by one.
                So you can do many more by firing a lot of different actions/events. Play around a little bit and find
                out if you can take this process up to payment form filling.
            </div>

            <div class="txt">
                I guess you can see that the things we can do with the help of “Selenium” are innumerable. You can use
                “Selenium” for scraping as it was expected. Let’s scrape prices of items we saw earlier, for that try this
                snippet:
            </div>

            <pre><code class="language-py">names = browser.find_elements_by_xpath('//a[@class="product--title"]')
prices = browser.find_elements_by_xpath('//span[@class="price--default is--nowrap"]')

name_price_dict = dict()

for name, price in zip(names, prices):
    name_price_dict[name.text] = price.text</code></pre>

            <div class="txt">
                Here we are using “find_elements_by_xpath” method, the name says why. This gets the “XPath” for matching all
                desired elements after and then returns a list of elements matching that criterion, which we iterate
                through and make a simple dict with key-name and value-price pairs.
            </div>





        </div>
    </div>
    

    <div class="container">
        <div class="resources">
            <div class="resources-title">Resources</div>
            <div id="resource-items" class="resources-list">
                <!-- generates -->
            </div>
        </div>
    </div>



    <div class="footer">
         Built by: <a href="mailto:tatevik.hakobyan.95@gmail.com">Tatevik Hakobyan</a> , <a href="mailto:Sedamarcosyan96@gmail.com">Seda Markosyan</a> and <a href="mailto:hovhannes.tamoyan@gmail.com">Hovhannes Tamoyan</a> &#169;
    </div>
        <script src="assets/js/prism.js"></script>
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" crossorigin="anonymous"></script>
    <script src="assets/js/main.js"></script>
</body>


</html>